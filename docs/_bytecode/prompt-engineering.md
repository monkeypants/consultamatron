Prompt engineering has two directions with opposite optimisation targets. Inbound (human → LLM): compile to semantic bytecode — dense, structured, precise, stripped of everything the LLM does not need. A prompt is a program; every non-contributing token is economic waste.

Token economics: three costs compound — financial (per-token pricing), attention (noise competes with signal for attention weight), displacement (noise crowds out useful context). The semantic waist is the architectural countermeasure: route bookkeeping through deterministic CLI calls at zero token cost.

Information architecture over density: progressive disclosure (metadata at startup, instructions on activation, resources on reference). The SKILL.md description field is an API contract for skill activation, not documentation. Context management: context rot (N tokens → N² attention relationships), context engineering (optimise the full information environment), scaling rules in prompts (calibrate agent effort explicitly).

Outbound (LLM → human): style is not decoration but a functional affordance for human cognition. It activates socio-cultural pattern matching, threat assessment, professional identity. Business before pleasure: semantic content verified first, style applied second. The directional model: compile inbound, style outbound, semantic waist between them.
