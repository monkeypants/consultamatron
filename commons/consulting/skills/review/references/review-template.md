# Post-Implementation Review — {Project Slug}

**Client**: {org-slug}
**Project**: {project-slug}
**Skillset**: {skillset name}
**Review date**: {date}
**Reviewed by**: {consultant}

## Engagement Summary

{Brief description of the project: scope, objectives, what was produced.}

## Timeline

| Date | Stage | Event |
|------|-------|-------|
| {date} | {skill} | {what happened — gate reached, revision, decision} |

## Per-Stage Assessment

### {Stage name} ({skill name})

**Outcome**: {completed / skipped / partial}
**Revision cycles**: {count of drafts or iterations}
**Gate artifact**: {path, or "none"}

**What worked**:
{What the skill did well at this stage. Specific examples.}

**What didn't work**:
{Where the skill struggled, produced wrong output, or required heavy
manual intervention. Specific examples.}

**Evidence**:
{References to specific artifacts, draft counts, decisions.md entries
that support the assessment.}

---

{Repeat for each stage in the pipeline}

## Process Observations

Verbatim from the consultant interview, Phase A.

### Process quality

{Verbatim responses about which stages were productive vs grinding.}

### Gaps

{Verbatim responses about what the client needed that no skill addressed.}

### Manual work

{Verbatim responses about work done outside the skills.}

### Research quality

{Verbatim responses about what was missing or wrong in org-research.}

### Client interaction

{Verbatim responses about how the agree loop worked in practice.}

### Surprises

{Verbatim responses about the unexpected and what would be done
differently.}

## Ideation Harvest

Verbatim from the consultant interview, Phase B. Grouped by theme.

### New skills

{Ideas for entirely new skills or capabilities.}

### Deliverable enhancements

{Ideas for improving existing deliverables or adding new artifact types.}

### Adjacent artifacts

{Ideas for complementary analyses: financial models, roadmaps, team
structures, risk registers, etc.}

### Competitive gaps

{What competitors or other frameworks offer that Consultamatron does not.}

### Client requests

{Things the client asked for that were out of scope but could become
in-scope.}

### Other ideas

{Anything that didn't fit the above categories.}

## Findings

### {Finding title}

**Category**: {process-fix / skill-gap / new-capability}
**Severity**: {high / medium / low}
**Affected skills**: {list of skills}

**Description**:
{What was observed and why it matters.}

**Recommendation**:
{Specific change to make — which skill, which step, what to add/change.}

**GitHub action**: {new issue / comment on #{number} / skip}

---

{Repeat for each finding}

## Cross-Project Patterns

{Only if multiple projects were reviewed in this engagement. Patterns
that span projects: shared pain points, systemic issues, opportunities
that no single project reveals.}
